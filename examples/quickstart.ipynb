{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI-Ethica Quick Start Guide\n",
        "\n",
        "This notebook demonstrates the core features of AI-Ethica: bias detection, fairness metrics, and transparency analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ai-ethica (uncomment if needed)\n",
        "# !pip install ai-ethica\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import AI-Ethica modules\n",
        "from ai_ethica import BiasDetector, FairnessMetrics, TransparencyAnalyzer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Bias Detection\n",
        "\n",
        "Detect bias in datasets across protected attributes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample dataset with potential bias\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    'feature1': np.random.randn(n_samples),\n",
        "    'feature2': np.random.randn(n_samples),\n",
        "    'gender': np.random.choice(['M', 'F'], n_samples, p=[0.7, 0.3]),  # Imbalanced\n",
        "    'race': np.random.choice(['A', 'B', 'C'], n_samples, p=[0.5, 0.3, 0.2]),\n",
        "    'target': np.random.choice([0, 1], n_samples, p=[0.6, 0.4])\n",
        "})\n",
        "\n",
        "# Initialize bias detector\n",
        "detector = BiasDetector()\n",
        "\n",
        "# Analyze bias\n",
        "bias_report = detector.analyze(\n",
        "    data=data,\n",
        "    protected_attributes=['gender', 'race'],\n",
        "    target_column='target'\n",
        ")\n",
        "\n",
        "print(f\"Dataset size: {bias_report['dataset_size']}\")\n",
        "print(f\"\\nBias Metrics:\")\n",
        "for attr, metrics in bias_report['bias_metrics'].items():\n",
        "    print(f\"\\n  {attr}:\")\n",
        "    rep_bias = metrics['representation_bias']\n",
        "    print(f\"    Representation disparity ratio: {rep_bias['disparity_ratio']:.2f}\")\n",
        "    print(f\"    Is balanced: {rep_bias['is_balanced']}\")\n",
        "\n",
        "print(f\"\\nRecommendations:\")\n",
        "for rec in bias_report['recommendations']:\n",
        "    print(f\"  - {rec}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fairness Metrics\n",
        "\n",
        "Calculate fairness metrics to evaluate model fairness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample predictions with some bias\n",
        "np.random.seed(42)\n",
        "n_samples = 500\n",
        "\n",
        "y_true = np.random.choice([0, 1], n_samples)\n",
        "protected_attr = np.random.choice(['group_A', 'group_B'], n_samples, p=[0.6, 0.4])\n",
        "\n",
        "# Create biased predictions\n",
        "y_pred = np.zeros(n_samples)\n",
        "for i in range(n_samples):\n",
        "    if protected_attr[i] == 'group_A':\n",
        "        y_pred[i] = np.random.choice([0, 1], p=[0.4, 0.6])\n",
        "    else:\n",
        "        y_pred[i] = np.random.choice([0, 1], p=[0.7, 0.3])\n",
        "    if y_true[i] == 1:\n",
        "        y_pred[i] = y_true[i]  # Some correlation with true labels\n",
        "\n",
        "# Initialize fairness metrics\n",
        "metrics = FairnessMetrics()\n",
        "\n",
        "# Evaluate fairness\n",
        "fairness_results = metrics.evaluate(\n",
        "    y_true=y_true,\n",
        "    y_pred=y_pred,\n",
        "    protected_attributes={'gender': protected_attr}\n",
        ")\n",
        "\n",
        "print(\"Fairness Results:\")\n",
        "for attr, results in fairness_results.items():\n",
        "    print(f\"\\n  {attr}:\")\n",
        "    if 'demographic_parity' in results:\n",
        "        dp = results['demographic_parity']\n",
        "        print(f\"    Demographic Parity:\")\n",
        "        print(f\"      Parity ratio: {dp['parity_ratio']:.2f}\")\n",
        "        print(f\"      Is fair: {dp['is_fair']}\")\n",
        "    \n",
        "    if 'equal_opportunity' in results:\n",
        "        eo = results['equal_opportunity']\n",
        "        print(f\"    Equal Opportunity:\")\n",
        "        print(f\"      TPR violation: {eo['violation']:.3f}\")\n",
        "        print(f\"      Is fair: {eo['is_fair']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Transparency Analysis\n",
        "\n",
        "Assess model transparency and interpretability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create and train a model\n",
        "X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Initialize transparency analyzer\n",
        "analyzer = TransparencyAnalyzer()\n",
        "\n",
        "# Assess transparency\n",
        "transparency_report = analyzer.assess(\n",
        "    model=model,\n",
        "    X=X_test[:10],\n",
        "    feature_names=[f'feature_{i}' for i in range(5)],\n",
        "    has_documentation=True,\n",
        "    has_explanations=False\n",
        ")\n",
        "\n",
        "print(f\"Model Type: {transparency_report['model_type']}\")\n",
        "print(f\"Interpretability Score: {transparency_report['interpretability_score']:.2f}\")\n",
        "print(f\"Overall Transparency Score: {transparency_report['transparency_score']:.2f}\")\n",
        "\n",
        "print(\"\\nRecommendations:\")\n",
        "for rec in transparency_report['recommendations']:\n",
        "    print(f\"  - {rec}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
